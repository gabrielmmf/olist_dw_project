# ğŸ“¦ olist_dw_project

Projeto de construÃ§Ã£o de um **Data Warehouse (DW)** baseado no [Brazilian E-Commerce Public Dataset by Olist](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce). Toda a engenharia de dados Ã© realizada com **Apache Hop**, com armazenamento analÃ­tico em **ClickHouse** e visualizaÃ§Ã£o interativa via **Apache Superset**.

---

## ğŸ“ Estrutura do Projeto

A raiz do projeto Ã© um projeto Apache Hop vÃ¡lido contendo `project-config.json`, pipelines, workflows, metadados e dados organizados.

```plaintext
olist_dw_project/
â”œâ”€â”€ project-config.json              # ConfiguraÃ§Ã£o principal do projeto Apache Hop
â”œâ”€â”€ metadata/                        # Metadados do projeto (conexÃµes, variÃ¡veis, ambientes)
â”œâ”€â”€ pipelines/                       # Pipelines de transformaÃ§Ã£o (ETL)
â”‚   â”œâ”€â”€ transform_orders.hpl
â”‚   â”œâ”€â”€ transform_customers.hpl
â”‚   â”œâ”€â”€ transform_products.hpl
â”‚   â”œâ”€â”€ transform_reviews.hpl
â”‚   â”œâ”€â”€ transform_payments.hpl
â”‚   â”œâ”€â”€ generate_fato_vendas.hpl
â”‚   â””â”€â”€ ...
â”œâ”€â”€ workflows/                       # Workflows de orquestraÃ§Ã£o
â”‚   â””â”€â”€ main_etl_workflow.hwf
â”œâ”€â”€ original_dataset/                # Dataset bruto exportado do arquivo SQLite da Olist
â”‚   â”œâ”€â”€ olist_orders_dataset.csv
â”‚   â”œâ”€â”€ olist_order_items_dataset.csv
â”‚   â”œâ”€â”€ olist_customers_dataset.csv
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ transformed/                     # Arquivos CSV transformados prontos para carga no DW
â”‚   â”œâ”€â”€ dim_cliente.csv
â”‚   â”œâ”€â”€ dim_produto.csv
â”‚   â”œâ”€â”€ fato_vendas.csv
â”‚   â””â”€â”€ ...
â”œâ”€â”€ dw_model_pgmodeler/              # Modelo dimensional criado no pgModeler
â”‚   â”œâ”€â”€ modelo_estrelado.dbm
â”‚   â””â”€â”€ modelo_estrelado.png
â”œâ”€â”€ dw_clickhouse/                   # Scripts SQL para criaÃ§Ã£o das tabelas no DW
â”‚   â”œâ”€â”€ create_dim_cliente.sql
â”‚   â”œâ”€â”€ create_fato_vendas.sql
â”‚   â””â”€â”€ ...
â”œâ”€â”€ dashboards_superset/             # Dashboards criados no Superset (opcionalmente exportados)
â”‚   â””â”€â”€ dashboard_vendas.json
â””â”€â”€ docs/                            # DocumentaÃ§Ã£o, slides, relatÃ³rio final
    â”œâ”€â”€ relatorio.pdf
    â””â”€â”€ apresentacao.pptx
````

---

## ğŸ§± Arquitetura Geral

```plaintext
original_dataset/ âœ Apache Hop âœ transformed/ âœ ClickHouse âœ Superset
```

### Etapas:

* **ExtraÃ§Ã£o**: CSVs extraÃ­dos do SQLite da Olist
* **TransformaÃ§Ã£o**: pipelines Apache Hop para derivar dimensÃµes, medidas e tabelas limpas
* **Carga**: dados transformados sÃ£o importados no ClickHouse
* **VisualizaÃ§Ã£o**: dashboards com Apache Superset, conectados ao DW

---

## ğŸ” Pipelines e Workflows

### ğŸ§© Pipelines (`pipelines/`)

Cada pipeline lida com uma transformaÃ§Ã£o de dados especÃ­fica:

* `transform_orders.hpl`: limpa e deriva dados de pedidos
* `transform_customers.hpl`: extrai dados Ãºnicos de clientes
* `transform_products.hpl`: inclui categorias e limpa nomes
* `transform_reviews.hpl`: trata avaliaÃ§Ãµes e tempos de resposta
* `transform_payments.hpl`: agrega tipos de pagamento por pedido
* `generate_fato_vendas.hpl`: cria a tabela fato centralizada
* Outros pipelines podem ser adicionados conforme necessidade

### ğŸ” Workflow (`workflows/`)

* `main_etl_workflow.hwf`: orquestra a execuÃ§Ã£o de todos os pipelines na sequÃªncia correta, da extraÃ§Ã£o Ã  geraÃ§Ã£o final dos arquivos em `transformed/`

---

## ğŸ§  Modelagem Dimensional

O DW foi modelado em **esquema estrela**, com base nos 4 passos de Kimball:

### 1. Processo de negÃ³cio modelado:

* **Vendas (pedidos com itens)**

### 2. Granularidade:

* Um registro por **item de pedido vendido**

### 3. DimensÃµes utilizadas:

* `dim_cliente` (cliente)
* `dim_produto` (produto + categoria)
* `dim_vendedor` (vendedor)
* `dim_data` (datas de compra e entrega)
* `dim_pagamento` (formas de pagamento)
* `dim_avaliacao` (score, tempo de resposta)

### 4. Fatos numÃ©ricos:

* Quantidade por item (`order_item_id`)
* PreÃ§o (`price`)
* Frete (`freight_value`)
* Total (`price + freight`)
* Dias de entrega (`entrega - compra`)
* Score da avaliaÃ§Ã£o

Diagrama criado em `pgModeler` disponÃ­vel em `dw_model_pgmodeler/modelo_estrelado.png`

---

## ğŸš€ Executando o Projeto

### 1. Clonar o repositÃ³rio

```bash
git clone https://github.com/seuusuario/olist_dw_project.git
cd olist_dw_project
```

### 2. Instalar o Apache Hop

* Site oficial: [https://hop.apache.org/download/](https://hop.apache.org/download/)
* Extraia e execute o `hop-gui`

### 3. Abrir o projeto no Hop

1. VÃ¡ em `Project â†’ Open Project`
2. Selecione o diretÃ³rio que contÃ©m o `project-config.json` (a raiz deste projeto)

### 4. Rodar o workflow principal

* Abra `workflows/main_etl_workflow.hwf`
* Clique em "Run" (ou pressione `F8`)
* Os arquivos finais serÃ£o gerados em `transformed/`

---

## ğŸ§¾ Tabelas no DW

### Tabela Fato

| Nome          | DescriÃ§Ã£o                               |
| ------------- | --------------------------------------- |
| `fato_vendas` | Registro de cada item de pedido vendido |

### DimensÃµes

| DimensÃ£o        | Atributos principais                       |
| --------------- | ------------------------------------------ |
| `dim_cliente`   | cidade, estado, ID do cliente              |
| `dim_produto`   | categoria, nome da categoria traduzida     |
| `dim_vendedor`  | cidade e estado do vendedor                |
| `dim_data`      | data, mÃªs, ano, dia da semana              |
| `dim_pagamento` | tipo de pagamento, parcelas, valor         |
| `dim_avaliacao` | score, tempo de resposta, comentÃ¡rio limpo |

---

## ğŸ“Š VisualizaÃ§Ã£o (Superset)

* Conecte o Superset ao banco ClickHouse com as tabelas geradas
* Crie dashboards com mÃ©tricas como:

  * Volume de vendas por categoria
  * Score mÃ©dio por estado
  * Tempo mÃ©dio de entrega por mÃªs
  * DistribuiÃ§Ã£o de tipos de pagamento

---

## ğŸ¤ Contribuindo

### PrÃ©-requisitos

* Apache Hop
* pgModeler (para editar o modelo ER)
* ClickHouse (ou outro DW analÃ­tico local)

### Regras para contribuiÃ§Ã£o

1. Use nomes consistentes para pipelines e arquivos
2. Coloque novas transformaÃ§Ãµes em `pipelines/`
3. Atualize o `main_etl_workflow.hwf` se necessÃ¡rio
4. Teste localmente com `Preview` antes de rodar a workflow
5. Documente qualquer alteraÃ§Ã£o no `README.md` ou crie um `CHANGELOG.md`

---

## ğŸ“˜ ReferÃªncias

* [Apache Hop Docs](https://hop.apache.org/docs/)
* [ClickHouse Docs](https://clickhouse.com/docs/)
* [Superset Docs](https://superset.apache.org/)
* [Olist Dataset no Kaggle](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)
